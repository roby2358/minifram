# Round One
- get the chat interface working: websockets
- Get some sort of http access to your model: GLM local
- Get some sort of data store working: SQLite?
- Figure out the MCP protocol, what the models send and what they expect in response
  - MCP tool display/configuration in the UI
- Then wire it all together in a service (Python? Typescript? Rust?)
  - Multi-round LLM invocation

# Round Two
- Button to spin up a new agent in a separate tab
- "Contract" box to define work
- Logic for the model to iterate until it feels the contract is complete

# Refactoring (2026-01-25)
- Extracted helper functions from websocket_endpoint to improve clarity:
  - build_tool_definitions(): Build tool list for LLM from MCP tools
  - execute_single_tool(): Execute one tool and get model's next response
  - process_tool_calls(): Loop through tool calls until model stops requesting them
  - send_final_response(): Extract reasoning and send final assistant message
- Reduced websocket handler from 225 lines to ~70 lines
- Logic remains unchanged, just better organized for maintainability